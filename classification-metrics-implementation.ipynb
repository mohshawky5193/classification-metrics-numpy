{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Binary Classification Metrics implementation using numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook I will implement some binary classification metrics like **accuracy**,**precision**,**recall** and **roc auc** using *numpy* only and then compare my implementation with *sklearn's* implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confusion Matrix\n",
    "\n",
    "|Label\\Prediction             | Positive Prediction | Negative Prediction         |\n",
    "| ---                         |    ---              |    ---                      |\n",
    "| **Positive Label**              | *TP*                | *FN*                        |\n",
    "| **Negative Label**              | *FP*                | *TN*                        |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://latex.codecogs.com/png.latex?Precision&space;=\\frac{tp}{tp&plus;fp}\" title=\"Precision =\\frac{tp}{tp+fp}\" />\n",
    "<img src=\"https://latex.codecogs.com/png.latex?Recall&space;=\\frac{tp}{tp&plus;fn}\" title=\"Recall =\\frac{tp}{tp+fn}\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Receiver Operator Curve\n",
    "![Receiver Operator Curve(ROC)](assets/roc.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "#Some assertions to check equal array sizes and to check that it is binary classification problem\n",
    "def assert_sizes(y_true,y_pred):\n",
    "    \n",
    "    assert(y_true.shape == y_pred.shape)\n",
    "    assert(np.unique(y_true).shape[0] == 2)\n",
    "    assert(np.max(y_true) == 1 and np.min(y_true)==0)\n",
    "    \n",
    "def get_y_thresholded(y_pred,threshold):\n",
    "    y_thresholded=np.copy(y_pred)\n",
    "    y_thresholded[y_thresholded >= threshold]=1\n",
    "    y_thresholded[y_thresholded < threshold]=0\n",
    "    return y_thresholded\n",
    "\n",
    "def my_accuracy(y_true,y_pred,threshold=0.5):\n",
    "    \n",
    "    assert_sizes(y_true,y_pred)\n",
    "    y_thresholded = get_y_thresholded(y_pred,threshold)\n",
    "    return np.sum(y_true == y_thresholded)/len(y_true)\n",
    "\n",
    "def my_precision(y_true,y_pred,threshold=0.5):\n",
    "    assert_sizes(y_true,y_pred)\n",
    "    y_thresholded = get_y_thresholded(y_pred,threshold)\n",
    "    condition1,condition2,condition3,condition4 = y_true==y_thresholded,y_true !=y_thresholded,y_true == 1,y_true == 0\n",
    "    tp = np.sum(np.logical_and(condition1,condition3))\n",
    "    fp = np.sum(np.logical_and(condition2,condition4))\n",
    "    \n",
    "    return tp/(tp+fp)\n",
    "\n",
    "def my_recall(y_true,y_pred,threshold=0.5):\n",
    "    assert_sizes(y_true,y_pred)\n",
    "    y_thresholded = get_y_thresholded(y_pred,threshold)\n",
    "    condition1,condition2,condition3,condition4 = y_true==y_thresholded,y_true !=y_thresholded,y_true == 1,y_true == 0\n",
    "    tp = np.sum(np.logical_and(condition1,condition3))\n",
    "    fn = np.sum(np.logical_and(condition2,condition3))\n",
    "    \n",
    "    return tp/(tp+fn)\n",
    "######################################################################################################################\n",
    "# Example Showing how ROC is drawn\n",
    "######################################################################################################################\n",
    "\n",
    "# <-|-x---x--o-x-o x----x---o--x-->\n",
    "\n",
    "# tpr = true_positives(number of positives on the right of the separator)/all_positives\n",
    "# fpr = false_positives(number of negatives on the right of the separator)/all_negatives\n",
    "\n",
    "# tpr = 3/3 =1\n",
    "# fpr = 6/6 =1\n",
    "\n",
    "#(1,1)\n",
    "\n",
    "# <---x-|-x--o-x-o x----x---o--x-->\n",
    "\n",
    "# tpr = 3/3 =1\n",
    "# fpr = 5/6\n",
    "#(5/6,1)\n",
    "\n",
    "# <---x---x|-o-x-o x----x---o--x-->\n",
    "\n",
    "#tpr = 3/3 =1\n",
    "#fpr = 4/6 =2/3\n",
    "\n",
    "#And so on then draw these points from bottom to top\n",
    "\n",
    "# then area under the curve is calculated by using trapezium rule\n",
    "def my_auc_roc(y_true,y_pred,threshold=0.5):\n",
    "    assert_sizes(y_true,y_pred)\n",
    "    y_pred_indices_sorted=np.argsort(y_pred)[::-1]\n",
    "    y_true = y_true[y_pred_indices_sorted]\n",
    "    all_positives = np.sum(y_true == 1)\n",
    "    all_negatives = np.sum(y_true == 0)\n",
    "    tps=np.cumsum(y_true)\n",
    "    fps=(np.arange(len(y_true))+1) - tps\n",
    "    tpr = tps / all_positives\n",
    "    fpr = fps / all_negatives\n",
    "    \n",
    "    return np.trapz(tpr,fpr)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sanity Checks\n",
    "> Now it's time to test our own implementation of binary metrics with sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All Correct\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score,precision_score,recall_score,roc_auc_score\n",
    "np.random.seed(42)\n",
    "y_true = np.array([1,0,1,1,0,0,1,0,1])\n",
    "y_pred = np.random.uniform(0,1,len(y_true))\n",
    "y_thresholded = get_y_thresholded (y_pred,0.5)\n",
    "#check correctness of implementation of accuracy\n",
    "assert (accuracy_score(y_true,y_thresholded) == my_accuracy(y_true,y_pred))\n",
    "\n",
    "#check correctness of implementation of precision\n",
    "assert (precision_score(y_true,y_thresholded) == my_precision(y_true,y_pred))\n",
    "\n",
    "#check correctness of implementation of recall\n",
    "assert (recall_score(y_true,y_thresholded) == my_recall(y_true,y_pred))\n",
    "\n",
    "#check correctness of implementation of auc\n",
    "\n",
    "assert (roc_auc_score(y_true,y_pred) == my_auc_roc(y_true,y_pred))\n",
    "\n",
    "print ('All Correct')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
